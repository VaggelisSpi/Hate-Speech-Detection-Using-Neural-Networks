{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 3338,
     "status": "ok",
     "timestamp": 1600735173614,
     "user": {
      "displayName": "vags S13",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiqvydqeKOhBMtVoM-vqMviNXQJ5rN3OSETagz9=s64",
      "userId": "04041356544312755286"
     },
     "user_tz": -180
    },
    "id": "zO1Hifib-Pka",
    "outputId": "b15870e3-3df5-4526-e43f-3f2dffd26976"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in /usr/local/lib/python3.6/dist-packages (0.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "executionInfo": {
     "elapsed": 2352,
     "status": "ok",
     "timestamp": 1600735173616,
     "user": {
      "displayName": "vags S13",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiqvydqeKOhBMtVoM-vqMviNXQJ5rN3OSETagz9=s64",
      "userId": "04041356544312755286"
     },
     "user_tz": -180
    },
    "id": "rk_834cR9T4B",
    "outputId": "158fa313-4caf-4cf8-9e82-5900877454c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import preprocessor as pr\n",
    "import spellcorrector\n",
    "import vectorization as vect\n",
    "import my_models\n",
    "import generator as gen\n",
    "import math\n",
    "import utils\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from utils import tweet_len\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rY0uMtI39T4J"
   },
   "source": [
    "Create the objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CRbIf7I09T4L"
   },
   "outputs": [],
   "source": [
    "preprocessor = pr.Preprocessor(stopwords=nltk_stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EcMJi-X9aSvj"
   },
   "outputs": [],
   "source": [
    "vec_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9XebnIdD9T4R"
   },
   "outputs": [],
   "source": [
    "vectorizer = vect.WordEmbeddingsVectorizer(embeddings_file='../MyData/glove.twitter.27B.200d.txt', vec_size=vec_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yTb6AmO99T4X"
   },
   "source": [
    "Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J1wSgAMS9T4Y"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../MyData/twitter_hate_data.csv\")\n",
    "print(\"Data shape:\", data.shape, \", data columns:\", data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Axq-GuCV9T4e"
   },
   "source": [
    "#### Split the data to train and test set respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cAqbi6O79T4g"
   },
   "outputs": [],
   "source": [
    "train_data, test_data, train_labels, test_labels = train_test_split(data[\"tweet\"], data[\"class\"], test_size=0.1,\n",
    "                                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "spwfDBPd9T4l"
   },
   "source": [
    "#### Preprocess train data and save it to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3lWFfMSz9T4n"
   },
   "outputs": [],
   "source": [
    "preprocessed_train_data = pd.DataFrame({'tweet': preprocessor.preprocess(train_data)})\n",
    "preprocessed_train_data[\"class\"] = train_labels\n",
    "preprocessed_train_data[\"length\"] = preprocessed_train_data.apply(lambda row: tweet_len(row), axis=1)\n",
    "preprocessed_train_data.sort_values(by=['length'], inplace=True, ignore_index=True)\n",
    "preprocessed_train_data.to_csv(\"../MyData/twitter_preprocessed_train_data.csv\", header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84gBpzFm9T4u"
   },
   "source": [
    "#### Preprocess test data and save it to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "husbnhJt9T4v"
   },
   "outputs": [],
   "source": [
    "preprocessed_test_data = pd.DataFrame({'tweet': preprocessor.preprocess(test_data)})\n",
    "preprocessed_test_data[\"class\"] = test_labels\n",
    "preprocessed_test_data[\"length\"] = preprocessed_test_data.apply(lambda row: tweet_len(row), axis=1)\n",
    "preprocessed_test_data.sort_values(by=['length'], inplace=True, ignore_index=True)\n",
    "preprocessed_test_data.to_csv(\"../MyData/twitter_preprocessed_test_data.csv\", header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dVNZv4jI9T41"
   },
   "source": [
    "##### Read from csv so we won't have to preprocess text each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z1YP88ST9T42"
   },
   "outputs": [],
   "source": [
    "preprocessed_train_data = pd.read_csv(\"../MyData/twitter_preprocessed_train_data.csv\", names=[\"index\", \"tweet\", \"class\",\n",
    "                                                                                              \"length\"])\n",
    "preprocessed_test_data = pd.read_csv(\"../MyData/twitter_preprocessed_test_data.csv\", names=[\"index\", \"tweet\", \"class\",\n",
    "                                                                                            \"length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yl2_hQ6kPfGF"
   },
   "outputs": [],
   "source": [
    "print(\"Train data shape:\", preprocessed_train_data.shape, \", data columns:\", preprocessed_train_data.columns)\n",
    "print(\"Test data shape:\", preprocessed_test_data.shape, \", data columns:\", preprocessed_test_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XiiXbNBf9T48"
   },
   "source": [
    "Change the classes from a string to categorical representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pgB6q5MW9T49"
   },
   "outputs": [],
   "source": [
    "train_labels = preprocessed_train_data[\"class\"]\n",
    "train_classes = pd.Series(train_labels, dtype=\"category\")\n",
    "le = LabelEncoder()\n",
    "train_categorical_classes = to_categorical(le.fit_transform(train_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "22fwrX7J9T5D"
   },
   "outputs": [],
   "source": [
    "test_labels = preprocessed_test_data[\"class\"]\n",
    "test_classes = pd.Series(test_labels, dtype=\"category\")\n",
    "le = LabelEncoder()\n",
    "test_categorical_classes = to_categorical(le.fit_transform(test_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mz1oyVnV9T5O"
   },
   "source": [
    "Initialize all the necessary variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3_4LN_I19T5P"
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "layers = 2\n",
    "go_backwards = True\n",
    "use_embeddings = False\n",
    "epochs = 20\n",
    "model_name = 'two_bi_glove_20ep_model'\n",
    "target_names = ['hate', 'neutral']\n",
    "num_classes = len(target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "01T-RIaKrlgm"
   },
   "source": [
    "Prepare the generators for the models with the embeddings layer and any other data that we will need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fvqbJanbrkOB"
   },
   "outputs": [],
   "source": [
    "# Make the word to id dictionary\n",
    "train_whole_text = utils.make_whole_text(preprocessed_train_data[\"tweet\"])\n",
    "test_whole_text = utils.make_whole_text(preprocessed_test_data[\"tweet\"])\n",
    "\n",
    "whole_text = train_whole_text + ' ' + test_whole_text\n",
    "\n",
    "word_list = whole_text.split()\n",
    "word_to_id = utils.build_vocab(word_list)\n",
    "\n",
    "vocab_size = len(word_to_id)\n",
    "\n",
    "# Get the length of the longest sequence\n",
    "lengths = [len(x) for x in preprocessed_train_data[\"tweet\"]]\n",
    "lengths = lengths + [len(x) for x in preprocessed_test_data[\"tweet\"]]\n",
    "maxlen = max(lengths)\n",
    "\n",
    "# Make the generator\n",
    "# 2 is the number of classes\n",
    "train_generator = gen.wordEmbeddingsGenerator(preprocessed_train_data[\"tweet\"],\n",
    "                                              train_categorical_classes,\n",
    "                                              len(preprocessed_train_data),\n",
    "                                              batch_size, 2, True,\n",
    "                                              vocab_size=vocab_size,\n",
    "                                              word_to_id=word_to_id,\n",
    "                                              maxlen=maxlen)\n",
    "\n",
    "test_generator = gen.wordEmbeddingsGenerator(preprocessed_test_data[\"tweet\"],\n",
    "                                             test_categorical_classes,\n",
    "                                             len(preprocessed_test_data),\n",
    "                                             batch_size, 2, True,\n",
    "                                             vocab_size=vocab_size,\n",
    "                                             word_to_id=word_to_id,\n",
    "                                             maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the model, train and evaluate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "executionInfo": {
     "elapsed": 121952,
     "status": "ok",
     "timestamp": 1600735371849,
     "user": {
      "displayName": "vags S13",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiqvydqeKOhBMtVoM-vqMviNXQJ5rN3OSETagz9=s64",
      "userId": "04041356544312755286"
     },
     "user_tz": -180
    },
    "id": "rVaT91a8mp7O",
    "outputId": "0c737697-bcb6-4e0b-ae9d-81242aca9638"
   },
   "outputs": [],
   "source": [
    "model = my_models.make_model(layers=layers, go_backwards=go_backwards, use_embeddings=use_embeddings,\n",
    "                             batch_size=batch_size, vec_size=vec_size, vocab_size=vocab_size, input_length=maxlen)\n",
    "model.load_weights('../MyModels/two_uni_layer_15ep_model-10.hdf5')\n",
    "\n",
    "x_train = preprocessed_train_data['tweet']\n",
    "y_train = train_categorical_classes\n",
    "\n",
    "x_test = preprocessed_test_data['tweet']\n",
    "y_test = test_categorical_classes\n",
    "\n",
    "print(\"Evaluating the model\")\n",
    "result = model.evaluate(test_generator.generate(), steps=math.ceil(test_size / batch_size), batch_size=batch_size)\n",
    "\n",
    "# Because our test data set does not divide exactly the batch size we will\n",
    "# extend it with data from the beginning so it can fit exactly the batch\n",
    "# size. We do this in order to make predictions for all the data. We will \n",
    "# then remove the extra data and make our reports with the initial test data\n",
    "test_data_tweets = []\n",
    "\n",
    "if use_embeddings:\n",
    "    for tweet in x_test:\n",
    "        test_data_tweets.append(rw.words_to_ids(tweet, word_to_id, maxlen))\n",
    "\n",
    "    for i in range(test_size, math.ceil(test_size / batch_size) * batch_size):\n",
    "        tweet = x_test.iloc[i - test_size]\n",
    "        test_data_tweets.append(rw.words_to_ids(tweet, word_to_id, maxlen))\n",
    "else:\n",
    "    max_len = len(x_test.iloc[-1])\n",
    "\n",
    "    for tweet in x_test:\n",
    "        test_data_tweets.append(vectorizer.vectorize(tweet, maxlen=max_len))\n",
    "\n",
    "    for i in range(test_size, math.ceil(test_size / batch_size) * batch_size):\n",
    "        tweet = x_test.iloc[i - test_size]\n",
    "        test_data_tweets.append(vectorizer.vectorize(tweet, maxlen=max_len))\n",
    "\n",
    "test_data_tweets = np.array(test_data_tweets)\n",
    "\n",
    "# Make the predictions\n",
    "print(\"Making the predictions\")\n",
    "steps = math.ceil(test_size / batch_size)\n",
    "preds = model.predict(test_data_tweets, batch_size=batch_size, verbose=1, steps=steps)\n",
    "\n",
    "y_true = np.argmax(y_test[:test_size], axis=1)\n",
    "y_pred = np.argmax(preds[:test_size], axis=1)\n",
    "\n",
    "# Print the report\n",
    "print(\"Making the report\")\n",
    "rw.print_report(result, y_true, y_pred, target_names, '../MyReports/' + model_name + '_report.txt')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "workflow.ipynb",
   "provenance": []
  },
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
