{"nbformat":4,"nbformat_minor":0,"metadata":{"jupytext":{"cell_metadata_filter":"-all","formats":"ipynb,py:light"},"kernelspec":{"display_name":"ThesisEnv","language":"python","name":"thesisenv"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"workflow.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"1Col6Irc9yAc","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1600735169852,"user_tz":-180,"elapsed":1588,"user":{"displayName":"vags S13","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqvydqeKOhBMtVoM-vqMviNXQJ5rN3OSETagz9=s64","userId":"04041356544312755286"}},"outputId":"00df967b-4bdc-4972-c9b8-f93551ee8099"},"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hklXkDLAAiF1","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1600735170346,"user_tz":-180,"elapsed":1306,"user":{"displayName":"vags S13","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqvydqeKOhBMtVoM-vqMviNXQJ5rN3OSETagz9=s64","userId":"04041356544312755286"}},"outputId":"dc906a44-185f-43ec-aa7d-58246e487ec8"},"source":["cd \"/content/drive/My Drive/Thesis_Project/src\""],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Thesis_Project/src\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zO1Hifib-Pka","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1600735173614,"user_tz":-180,"elapsed":3338,"user":{"displayName":"vags S13","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqvydqeKOhBMtVoM-vqMviNXQJ5rN3OSETagz9=s64","userId":"04041356544312755286"}},"outputId":"b15870e3-3df5-4526-e43f-3f2dffd26976"},"source":["!pip install emoji"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: emoji in /usr/local/lib/python3.6/dist-packages (0.6.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rk_834cR9T4B","colab":{"base_uri":"https://localhost:8080/","height":89},"executionInfo":{"status":"ok","timestamp":1600735173616,"user_tz":-180,"elapsed":2352,"user":{"displayName":"vags S13","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqvydqeKOhBMtVoM-vqMviNXQJ5rN3OSETagz9=s64","userId":"04041356544312755286"}},"outputId":"158fa313-4caf-4cf8-9e82-5900877454c4"},"source":["# Imports\n","import pandas as pd\n","import nltk\n","import tensorflow as tf\n","import numpy as np\n","from nltk.corpus import stopwords as nltk_stopwords\n","from sklearn.model_selection import train_test_split\n","\n","import preprocessor as pr\n","import spellcorrector\n","import vectorization as vect\n","import my_models\n","import generator as gen\n","import math\n","import utils\n","\n","from keras.callbacks import ModelCheckpoint\n","from keras.utils import to_categorical\n","from sklearn.preprocessing import LabelEncoder\n","from utils import tweet_len\n","from sklearn.metrics import classification_report, confusion_matrix\n","\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","\n","pd.set_option('display.max_columns', None)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rY0uMtI39T4J"},"source":["Create the objects"]},{"cell_type":"code","metadata":{"id":"CRbIf7I09T4L"},"source":["preprocessor = pr.Preprocessor(stopwords=nltk_stopwords.words('english'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EcMJi-X9aSvj"},"source":["vec_size = 200"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9XebnIdD9T4R"},"source":["vectorizer = vect.WordEmbeddingsVectorizer(embeddings_file='../MyData/glove.twitter.27B.200d.txt', vec_size=vec_size)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yTb6AmO99T4X"},"source":["Read the data"]},{"cell_type":"code","metadata":{"id":"J1wSgAMS9T4Y"},"source":["data = pd.read_csv(\"../MyData/twitter_hate_data.csv\")\n","print(\"Data shape:\", data.shape, \", data columns:\", data.columns)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Axq-GuCV9T4e"},"source":["#### Split the data to train and test set respectively"]},{"cell_type":"code","metadata":{"id":"cAqbi6O79T4g"},"source":["train_data, test_data, train_labels, test_labels = train_test_split(data[\"tweet\"], data[\"class\"], test_size=0.1,\n","                                                                    random_state=0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"spwfDBPd9T4l"},"source":["#### Preprocess train data and save it to a csv file"]},{"cell_type":"code","metadata":{"id":"3lWFfMSz9T4n"},"source":["preprocessed_train_data = pd.DataFrame({'tweet': preprocessor.preprocess(train_data)})\n","preprocessed_train_data[\"class\"] = train_labels\n","preprocessed_train_data[\"length\"] = preprocessed_train_data.apply(lambda row: tweet_len(row), axis=1)\n","preprocessed_train_data.sort_values(by=['length'], inplace=True, ignore_index=True)\n","preprocessed_train_data.to_csv(\"../MyData/twitter_preprocessed_train_data.csv\", header=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"84gBpzFm9T4u"},"source":["#### Preprocess test data and save it to a csv file"]},{"cell_type":"code","metadata":{"id":"husbnhJt9T4v"},"source":["preprocessed_test_data = pd.DataFrame({'tweet': preprocessor.preprocess(test_data)})\n","preprocessed_test_data[\"class\"] = test_labels\n","preprocessed_test_data[\"length\"] = preprocessed_test_data.apply(lambda row: tweet_len(row), axis=1)\n","preprocessed_test_data.sort_values(by=['length'], inplace=True, ignore_index=True)\n","preprocessed_test_data.to_csv(\"../MyData/twitter_preprocessed_test_data.csv\", header=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dVNZv4jI9T41"},"source":["##### Read from csv so we won't have to preprocess text each time"]},{"cell_type":"code","metadata":{"id":"z1YP88ST9T42"},"source":["preprocessed_train_data = pd.read_csv(\"../MyData/twitter_preprocessed_train_data.csv\", names=[\"index\", \"tweet\", \"class\",\n","                                                                                              \"length\"])\n","preprocessed_test_data = pd.read_csv(\"../MyData/twitter_preprocessed_test_data.csv\", names=[\"index\", \"tweet\", \"class\",\n","                                                                                            \"length\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yl2_hQ6kPfGF"},"source":["print(\"Train data shape:\", preprocessed_train_data.shape, \", data columns:\", preprocessed_train_data.columns)\n","print(\"Test data shape:\", preprocessed_test_data.shape, \", data columns:\", preprocessed_test_data.columns)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XiiXbNBf9T48"},"source":["Change the classes from a string to categorical representation"]},{"cell_type":"code","metadata":{"id":"pgB6q5MW9T49"},"source":["train_labels = preprocessed_train_data[\"class\"]\n","train_classes = pd.Series(train_labels, dtype=\"category\")\n","le = LabelEncoder()\n","train_categorical_classes = to_categorical(le.fit_transform(train_classes))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"22fwrX7J9T5D"},"source":["test_labels = preprocessed_test_data[\"class\"]\n","test_classes = pd.Series(test_labels, dtype=\"category\")\n","le = LabelEncoder()\n","test_categorical_classes = to_categorical(le.fit_transform(test_classes))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mz1oyVnV9T5O"},"source":["Initialize all the neccessary variables"]},{"cell_type":"code","metadata":{"id":"3_4LN_I19T5P"},"source":["batch_size = 256\n","layers = 1\n","go_backwards = True\n","use_embeddings = False\n","epochs = 20\n","model_name = 'one_bi_glove_20ep_model'\n","target_names = ['hate', 'neutral']\n","num_classes = len(target_names)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RhlMX8LbB43e"},"source":["train_size = len(preprocessed_train_data['tweet'])\n","test_size = len(preprocessed_test_data['tweet'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9BWNg3XFCuSj"},"source":["Create two generators that will generate the training and the testing dataset respectively"]},{"cell_type":"code","metadata":{"id":"zTjjL58MBt1L"},"source":["train_generator = gen.KerasBatchGenerator(preprocessed_train_data['tweet'], train_categorical_classes, train_size, batch_size, num_classes, vectorizer, vec_size)\n","test_generator = gen.KerasBatchGenerator(preprocessed_test_data['tweet'], test_categorical_classes, test_size, batch_size, num_classes, vectorizer, vec_size)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"01T-RIaKrlgm"},"source":["Prepare the generators for the models with the embeddings layer and any other data that we will need"]},{"cell_type":"code","metadata":{"id":"fvqbJanbrkOB"},"source":["import wordEmbeddingsGenerator as gen\n","\n","# Initialize the variables\n","vec_size = 200\n","batch_size = 256\n","use_embeddings = True\n","target_names = ['hate', 'neutral']\n","num_classes = len(target_names)\n","\n","# Make the word to id dictionary\n","train_whole_text = utils.make_whole_text(preprocessed_train_data[\"tweet\"])\n","test_whole_text = utils.make_whole_text(preprocessed_test_data[\"tweet\"])\n","\n","whole_text = train_whole_text + ' ' + test_whole_text\n","\n","word_list = whole_text.split()\n","word_to_id = utils.build_vocab(word_list)\n","\n","vocab_size = len(word_to_id)\n","\n","# Get the length of the longest sequence\n","lengths = [len(x) for x in preprocessed_train_data[\"tweet\"]]\n","lengths = lengths + [len(x) for x in preprocessed_test_data[\"tweet\"]]\n","maxlen = max(lengths)\n","\n","# Make the generator\n","# 2 is the number of classes\n","train_generator = gen.wordEmbeddingsGenerator(preprocessed_train_data[\"tweet\"],\n","                                              train_categorical_classes,\n","                                              len(preprocessed_train_data),\n","                                              batch_size, 2, True, \n","                                              vocab_size=vocab_size,\n","                                              word_to_id=word_to_id,\n","                                              maxlen=maxlen)\n","\n","test_generator = gen.wordEmbeddingsGenerator(preprocessed_test_data[\"tweet\"],\n","                                              test_categorical_classes,\n","                                              len(preprocessed_test_data),\n","                                              batch_size, 2, True, \n","                                              vocab_size=vocab_size,\n","                                              word_to_id=word_to_id,\n","                                              maxlen=maxlen)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gYexGZeMmQW5","colab":{"base_uri":"https://localhost:8080/","height":163},"executionInfo":{"status":"ok","timestamp":1600735495794,"user_tz":-180,"elapsed":123923,"user":{"displayName":"vags S13","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqvydqeKOhBMtVoM-vqMviNXQJ5rN3OSETagz9=s64","userId":"04041356544312755286"}},"outputId":"937b5791-99dc-409c-b39b-61ccd859c65c"},"source":["epochs = 4\n","layers = 2\n","go_backwards = True\n","model_name = 'two_bi_layer_15ep_model'\n","model = my_models.make_model(layers=layers, go_backwards=go_backwards, use_embeddings=use_embeddings, \n","                             batch_size=batch_size, vec_size=vec_size, vocab_size=vocab_size, input_length=maxlen)\n","model.load_weights('../MyModels/two_bi_layer_15ep_model-11.hdf5')\n","\n","x_train = preprocessed_train_data['tweet']\n","y_train = train_categorical_classes\n","\n","x_test = preprocessed_test_data['tweet']\n","y_test = test_categorical_classes\n","\n","\n","train_size = len(x_train)\n","test_size = len(x_test)\n","\n","print(\"Evaluating the model\")\n","result = model.evaluate(test_generator.generate(), steps=math.ceil(test_size / batch_size), batch_size=batch_size)\n","\n","# Because our test data set does not divide exactly the batch size we will\n","# extend it with data from the beggining so it can fit exactly the batch\n","# size. We do this in order to make predictions for all the data. We will \n","# then remove the extra data and make our reports with the initial test data\n","test_data_tweets = []\n","\n","if use_embeddings:\n","    for tweet in x_test:\n","        test_data_tweets.append(rw.words_to_ids(tweet, word_to_id, maxlen))\n","\n","    for i in range(test_size, math.ceil(test_size / batch_size)*batch_size):\n","        tweet = x_test.iloc[i - test_size]\n","        test_data_tweets.append(rw.words_to_ids(tweet, word_to_id, maxlen))\n","else:\n","    max_len = len(x_test.iloc[-1])\n","\n","    for tweet in x_test:\n","        test_data_tweets.append(vectorizer.vectorize(tweet, maxlen=max_len))\n","\n","    for i in range(test_size, math.ceil(test_size / batch_size)*batch_size):\n","        tweet = x_test.iloc[i - test_size]\n","        test_data_tweets.append(vectorizer.vectorize(tweet, maxlen=max_len))\n","\n","test_data_tweets = np.array(test_data_tweets)\n","\n","# Make the predictions\n","print(\"Making the predictions\")\n","steps=math.ceil(test_size / batch_size)\n","preds = model.predict(test_data_tweets, batch_size=batch_size, verbose=1, steps=steps)\n","\n","y_true = np.argmax(y_test[:test_size], axis=1)\n","y_pred = np.argmax(preds[:test_size], axis=1)\n","\n","# Print the report\n","print(\"Making the report\")\n","rw.print_report(result, y_true, y_pred, target_names, '../MyReports/' + model_name + '_report.txt')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Evaluating the model\n","10/10 [==============================] - 53s 5s/step - loss: 0.6931 - categorical_accuracy: 0.8355\n","Making the predictions\n","10/10 [==============================] - 53s 5s/step\n","Making the report\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"rVaT91a8mp7O","colab":{"base_uri":"https://localhost:8080/","height":107},"executionInfo":{"status":"ok","timestamp":1600735371849,"user_tz":-180,"elapsed":121952,"user":{"displayName":"vags S13","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqvydqeKOhBMtVoM-vqMviNXQJ5rN3OSETagz9=s64","userId":"04041356544312755286"}},"outputId":"0c737697-bcb6-4e0b-ae9d-81242aca9638"},"source":["layers = 2\n","epochs = 5\n","go_backwards = False\n","model_name = 'two_uni_layer_15ep_model'\n","\n","model = my_models.make_model(layers=layers, go_backwards=go_backwards, use_embeddings=use_embeddings, \n","                             batch_size=batch_size, vec_size=vec_size, vocab_size=vocab_size, input_length=maxlen)\n","model.load_weights('../MyModels/two_uni_layer_15ep_model-10.hdf5')\n","\n","x_train = preprocessed_train_data['tweet']\n","y_train = train_categorical_classes\n","\n","x_test = preprocessed_test_data['tweet']\n","y_test = test_categorical_classes\n","\n","\n","train_size = len(x_train)\n","test_size = len(x_test)\n","\n","\n","print(\"Evaluating the model\")\n","result = model.evaluate(test_generator.generate(), steps=math.ceil(test_size / batch_size), batch_size=batch_size)\n","\n","# Because our test data set does not divide exactly the batch size we will\n","# extend it with data from the beggining so it can fit exactly the batch\n","# size. We do this in order to make predictions for all the data. We will \n","# then remove the extra data and make our reports with the initial test data\n","test_data_tweets = []\n","\n","if use_embeddings:\n","    for tweet in x_test:\n","        test_data_tweets.append(rw.words_to_ids(tweet, word_to_id, maxlen))\n","\n","    for i in range(test_size, math.ceil(test_size / batch_size)*batch_size):\n","        tweet = x_test.iloc[i - test_size]\n","        test_data_tweets.append(rw.words_to_ids(tweet, word_to_id, maxlen))\n","else:\n","    max_len = len(x_test.iloc[-1])\n","\n","    for tweet in x_test:\n","        test_data_tweets.append(vectorizer.vectorize(tweet, maxlen=max_len))\n","\n","    for i in range(test_size, math.ceil(test_size / batch_size)*batch_size):\n","        tweet = x_test.iloc[i - test_size]\n","        test_data_tweets.append(vectorizer.vectorize(tweet, maxlen=max_len))\n","\n","test_data_tweets = np.array(test_data_tweets)\n","\n","# Make the predictions\n","print(\"Making the predictions\")\n","steps=math.ceil(test_size / batch_size)\n","preds = model.predict(test_data_tweets, batch_size=batch_size, verbose=1, steps=steps)\n","\n","y_true = np.argmax(y_test[:test_size], axis=1)\n","y_pred = np.argmax(preds[:test_size], axis=1)\n","\n","# Print the report\n","print(\"Making the report\")\n","rw.print_report(result, y_true, y_pred, target_names, '../MyReports/' + model_name + '_report.txt')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Evaluating the model\n","10/10 [==============================] - 52s 5s/step - loss: 0.4477 - categorical_accuracy: 0.9387\n","Making the predictions\n","10/10 [==============================] - 52s 5s/step\n","Making the report\n"],"name":"stdout"}]}]}